---
title: "Practical 2"
author: "YOUR NAME HERE"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)
# load libraries
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(tidyr)


# import the data file using the readr package
#az_data <- readr::read_csv("az_products_practical.csv")

# or 
az_data <- read.csv("C:/Users/bhojr/OneDrive/Desktop/rigor_practical 2/rigor_practical/review_data.csv")

```

## Purpose
The purpose of this practical is to assess student mastery of verifying the correctness of their code. 


## Instructions
For each of the following questions, provide your written answer in complete sentences in the text area of the R Markdown report and put your code in the code chunks. Unless otherwise specified, set your code chunk options to have the code and output shown but no messages or warnings. You may not ask anyone but Dr. Lapato for clarification or help. If you use any online resources, you must list those resources in a bulleted list beside the relevant question. Failure to do so may constitute an Honor code violation.


## Submission
Practical 2 is due by November 19th at 11:59PM. Submit your Rmd and rendered PDF as a zip file to Canvas. Late submissions will lose 1% credit for each hour late. 

## Question 0 
Please type out the VCU Honor Code and then type your name.


## Question 1 (20 points)
A csv file and a data dictionary have been provided by a collaborator.  These files are from a dataset of Amazon product reviews. Import the dataset into R (show your code in this R Markdown report) and read over the data dictionary. As you know, data dictionaries give analysts information about the dataset that can be used during exploratory data analysis to assess data quality control. 

Name two specific data quality checks you might perform on this dataset based on information provided in the data dictionary and what you see when you look at the dataset. Specify what information in the data dictionary prompted you to make this check and name the function(s) you would use to make the evaluation. 

* Specific data quality check #1:There are many specific data checks like Accuracy,Precision,Completeness,Validity,Relevancy
Timeliness etc.Based on the information given in the data disctionary I choose summerize function from the dplyr package to calculate the median value of rating variable and maximum price from the price variable that summerises their rating and prices of rows.
```{r}

    az_data %>% 
    summerize (count_median = median(rating)) # summerises the total value of rating
     count_max = max(price) # summerises the maximum price

```

* Specific data quality check #2:The data quality check in this section I choose rearrange function from the dplyr package to order the dataset rows in descending order based on the column value of arin variable,because this variable represent the unique reviews ID from maximum to minimum.

```{r}
arrange(data,desc(arin))# order the row in descending order of the review's ID
```

**Rubric**

* Two correct specific data quality checks are mentioned (5 points)
* Correct methods for performing these checks are provided (10 points)
* Data is imported correctly (5 points)

```
Example of an answer using the `babynames` dataset:
```
The data dictionary states that the dataset only "includes names with at least 5 uses", so I would use `summarize()` from the dplyr package to calculate the minimum `n` for the dataset to verify that no values less than 5 are present for the count variable.



<br>

## Question 2 (30 points)


There are many ways to clean a character string variable. Below, there are four approaches one could use to clean a numeric variable that happens to include some string-like characters (e.g., $). 

**Part A (15 points)**  

Use `dplyr::mutate` to create four new variables in your combined dataset that clean the `price` variable (one new variable for each method). Please name your variables: `price_numeric`, `price_parse`, `price_stringr`, and `price_gsub`. Remember that proper tidyverse code does not use dollar signs for variable selection. One method has been coded for you as an example.

* Method 1 (simple base R): `as.numeric(az_data_combined$price)`
* Method 2 (tidyverse, readr): `readr::parse_number(az_data_combined$price)`
* Method 3 (tidyverse, stringr): `as.numeric(stringr::str_match(az_data_combined$price, "[0-9]+"))`
* Method 4 (base R, regex): `as.numeric(gsub('[$,]', '', az_data_combined$price))`. 


**Part B (15 points)**  
Add code annotations that describe how each method is cleaning the `price` variable. One method has been annotated for you as an example.  


**Move the example code and annotation into a proper code chunk and adjust the code chunk options so that only the code shows in the knitted report (i.e., no dataset printout or summary).**

**Rubric**

* New variables are added correctly (15 points)
* Code annotations are correct (5 points)
* Code annotations are specific (5 points)
* Code shows but the dataset does not (5 points)

```
<DATASET> %>% 
  mutate(
         price_gsub = as.numeric(gsub('[$,]', '', price)) # This method removes dollar 
         signs from a string by replacing the dollar sign with an empty string. 
         )
```


```{r question2}

```


## Question 3 (20 points)

Create a table that summarizes key descriptives of each cleaning method.  Your table must be created with a function (not manually) and should include the minimum value, maximum value, and mean value for each cleaning method. Round all numeric values to two decimal places. You do not have to use the tidyverse to earn full credit; however, folks who are able to create a table with the structure described below using only tidyverse functions will be eligible for up to 30 bonus points. 

The table structure that must be used to earn full credit and any bonus points is 1) the cleaning methods as column names, and 2) the descriptive statistic as a row. The order of the summary statistics (rows) and cleaning methods (columns) do not matter. `999` was used below to illustrate where numeric values would go. 

| Statistic | numeric | readr | stringr | gsub |
|----|----|----|----|
| mean | 999 | 999 | 999 | 999 |
| min | 999 | 999 | 999 | 999 |
| max | 999 | 999 | 999 | 999 |


Thirty (30) bonus points are available for correctly using the following functions in your calculation and table creation (partial extra credit is possible):

* summarize_at() [10 points]
* pivot_longer() [5 points]
* pivot_wider()  [5 points]
* names_pattern argument in pivot_longer() [10 points]

You have all of the tools you need to earn these bonus points in the examples provided in the class slides, the R help documentation, and the online [`dplyr` resource](https://dplyr.tidyverse.org/reference).


**Rubric**

* Table structure is correct (10 points)
* Table values are correct (10 points)


```{r question3}

```



## Question 4 (20 points)

Explain why there are differences in the summary statistics for each method. To earn full credit, you must explicitly link how the method wrangles data to how that action impacts what values are altered or set to missing. 


**Rubric**

* At least three specific differences in output are discussed (5 points)
* The discussion links method to impact (15 points)


<br>

## Question 5 (60 points)

**Part A.** (20 points)
Which method is best? Explain your rationale. Be sure to consider how well each method preserves the data (e.g., How much information does the method lose?).
 

**Rubric**

* Student selects a reasonable method as "best" (5 points)
* Student provides specific evidence supporting their choice (15 points)


**Part B.** (30 points)
Were any raw data values lost (i.e., set to missing) by all four methods? If you believe that there were raw data values lost (i.e., set to missing) by all four methods, how do you know? What caused these values to be set to missing by all four methods? How problematic is this loss of data? 

If you believe that there no raw data values were lost (i.e., set to missing) by all four methods, how do you know? Which method lost the least amount of data?


**Rubric**

* Student correctly states if any raw data values or formats were lost (10 points)
* Student provides a thorough explanation of their reasoning (15 points)
* Student discusses either impact of data loss or correctly names which method lost the least amount of data.

**Part C.** (10 points)
Is there a method or a combination of methods that would preserve the data better? Explain your reasoning.

**Rubric**

* Student provides a thoughtful, reasonable answer supported by evidence (10 points)

